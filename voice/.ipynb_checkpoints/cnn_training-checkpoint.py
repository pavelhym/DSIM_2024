# -*- coding: utf-8 -*-
"""CNN_DSIM_Project_Monochannelsignal.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18DfA-cU-8J5mLHf55l93odcsPRHLthV-
"""

!pip install np_utils &> /dev/null

import numpy as np
import matplotlib.pyplot as plt
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report, confusion_matrix
import joblib
from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import OneHotEncoder

import keras
from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout, BatchNormalization

from google.colab import drive
drive.mount('/content/drive')

X_train = np.load('/content/drive/MyDrive/DSIM_Project_Monochannelsignal/extracted_features_X_train.npy')
X_test = np.load('/content/drive/MyDrive/DSIM_Project_Monochannelsignal/extracted_features_X_test.npy')
y_train = np.load('/content/drive/MyDrive/DSIM_Project_Monochannelsignal/extracted_features_y_train.npy')
y_test = np.load('/content/drive/MyDrive/DSIM_Project_Monochannelsignal/extracted_features_y_test.npy')

# Convert everything into numpy arrays for subsequent neural processing
X_train = np.array(X_train)
y_train = np.array(y_train)
X_test = np.array(X_test)
y_test = np.array(y_test)

enc = OneHotEncoder()

#X_train = np.reshape(X_train, newshape=((X_train.shape[0], 150, 182)))
#X_test = np.reshape(X_test, newshape=((X_test.shape[0], 150,182)))
y_train = enc.fit_transform(np.array(y_train).reshape(-1,1)).toarray()
y_test = enc.transform(np.array(y_test).reshape(-1,1)).toarray()

print(X_train.shape)
print(y_train.shape)
print(len(y_train[1]))

# making our data compatible to model.
X_train = np.expand_dims(X_train, axis=2)
X_test = np.expand_dims(X_test, axis=2)
X_train.shape, y_train.shape, X_test.shape, y_test.shape

input = keras.Input((X_train.shape[1], 1))
x = keras.layers.Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu', input_shape=((X_train.shape[1], 1)))(input)
x = keras.layers.MaxPooling1D(pool_size=5, strides = 2, padding = 'same')(x)

x = keras.layers.Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu')(x)
x = keras.layers.MaxPooling1D(pool_size=5, strides = 2, padding = 'same')(x)

x = keras.layers.Conv1D(128, kernel_size=5, strides=1, padding='same', activation='relu')(x)
x = keras.layers.MaxPooling1D(pool_size=5, strides = 2, padding = 'same')(x)
x = keras.layers.Dropout(0.2)(x)

x = keras.layers.Conv1D(64, kernel_size=5, strides=1, padding='same', activation='relu')(x)
x = keras.layers.MaxPooling1D(pool_size=5, strides = 2, padding = 'same')(x)
x = keras.layers.Dropout(0.2)(x)

x = keras.layers.Flatten()(x)
x = keras.layers.Dense(32, activation = 'relu')(x)
x = keras.layers.Dropout(0.3)(x)
output = keras.layers.Dense(8, activation = 'softmax')(x)
model = keras.Model(input, output)

model.compile(optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = ['categorical_accuracy'])

model.summary()

history=model.fit(X_train, y_train, batch_size=64, epochs=40, validation_data=(X_test, y_test))

# Visualization of the learning curves
# Loss
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend(['train', 'test'])
# Accuracy
plt.subplot(1, 2, 2)
plt.plot(history.history['categorical_accuracy'])
plt.plot(history.history['val_categorical_accuracy'])
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend(['train', 'test'])

pred_test = model.predict(X_test)
y_pred = enc.inverse_transform(pred_test)

y_test = enc.inverse_transform(y_test)

# Confusion matrix
print('Confusion matrix:')
cm = confusion_matrix(y_test, y_pred)
plt.imshow(cm, cmap=plt.cm.Blues);
plt.xlabel('Prediction');
plt.ylabel('Ground truth');

print(classification_report(y_test, y_pred))

joblib.dump(model, '/content/drive/My Drive/DSIM_Project_Monochannelsignal/cnn_model.joblib') # Saving the model

